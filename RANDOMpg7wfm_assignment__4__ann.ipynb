{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parulgo7/MLNotebooks/blob/main/RANDOMpg7wfm_assignment__4__ann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-40VPC7MAGGB"
      },
      "source": [
        "# Assignment 4: Benchmarking Neural Nets with the XOR Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piFzh10hAGGE"
      },
      "source": [
        "### CS 4774 Machine Learning - Department of Computer Science - University of Virginia\n",
        "In this assignment, you will implement your own neural networks to classify non-linear data from the XOR dataset. For references, you may refer to my [lecture 13](https://docs.google.com/presentation/d/1otQfmMomWctLZKI3hHKAA4lLkbXFtagLaQov8gNh4LI/edit?usp=sharing) and [Colab Notebook 10](https://colab.research.google.com/drive/1x5biI3dP5YvvDEI0wapJcSgQNnATDzNe) if you need additional sample codes to help with your assignment. For deliverables, you must write code in Python/Tensorflow and submit **this** Jupyter Notebook file (.ipynb) to earn a total of 100 pts. Note that you must save your Notebook filename under this format: **yourUvaUserId_assignment_4_ann.ipynb**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "469YvvIzAGGJ"
      },
      "source": [
        "# You might want to use the following packages\n",
        "import sklearn\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PtpH4xAGGG"
      },
      "source": [
        "---\n",
        "## 1. THE DATASET AND VISUALIZATION\n",
        "\n",
        "We will use the non-linear toy data called the XOR dataset. You may use the code snippet below to generate the train/validate/test set. Feel free to change the number of samples, and noise level. To keep the ratio between the sets consistent, please do not change the test_size and random_state parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6To8trtr3sLh"
      },
      "source": [
        "def make_xor(n_points):\n",
        "    centers = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "    labels = np.array([0,1,1,0])\n",
        "    data = np.array([]).reshape(-1,3)\n",
        "    for center, label in zip(centers,labels):\n",
        "        points = np.random.normal(loc=center,scale=0.2,size=(n_points//4,2))\n",
        "        points_labels = np.hstack((points,label*np.ones(n_points//4).reshape((-1, 1))))\n",
        "        data = np.vstack((data,points_labels))\n",
        "    return (data[:,[0,1]],data[:,2])\n",
        "\n",
        "\n",
        "X, y = make_xor(1000)\n",
        "y=y.astype(np.int64)\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=49)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size= 0.1, random_state=49)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uUeN8qXQPEO"
      },
      "source": [
        "\n",
        "---\n",
        "## 2. TRAIN A SIMPLE ANN FOR CLASSIFICATION TASK\n",
        "\n",
        "Use the standard libarary of Neural Net on the training data, and then test the classifier on the test data. You will create a simple ANN with 3 layers: an Input Layer, a Hidden Layer and an Output Layer. For each layer, you can specify the number of nodes appropriate for the XOR problem. Also, feel free to tune the network as you see fit. You have to report the accuracy of the network on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GopK02xaQXoH"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "my_model = keras.Sequential()\n",
        "# Your code here!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "rctwTuLnbMEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[2]),\n",
        "    keras.layers.Dense(200, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "tYG9uYb-a6IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.summary()"
      ],
      "metadata": {
        "id": "PkaSWG1xbJJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I confirm if my numbers make sense! The flatten layer is as it should be (720*2)."
      ],
      "metadata": {
        "id": "kGdjJsHxccRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.compile(loss=\"sparse_categorical_crossentropy\" ,\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "hmEw-sL7c0wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = my_model.fit(X_train, y_train, batch_size=20, epochs=30,\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "LczV_juSbTfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my_model2 = keras.models.Sequential([\n",
        "#     keras.layers.Flatten(input_shape=[2]),\n",
        "#     keras.layers.Dense(200, activation=\"relu\"),\n",
        "#     keras.layers.Dense(300, activation=\"softmax\")\n",
        "# ])\n",
        "\n",
        "my_model2 = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[2]),\n",
        "    keras.layers.Dense(200, activation=\"relu\"),\n",
        "    keras.layers.Dense(2, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "Vb__MRKdAd0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model2.summary()"
      ],
      "metadata": {
        "id": "OQzNYKLsAiLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model2.compile(loss=\"sparse_categorical_crossentropy\" ,\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "tGnDSZBIAlzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = my_model2.fit(X_train, y_train, batch_size=20, epochs=30,\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "nrKSbYVzAonM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see the val_accuracy of the second set to be better so we will go with those node number values based off of our by hand tuning!"
      ],
      "metadata": {
        "id": "rM9JjJV8Bwu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "C5VhTaJTHh29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The steps we need to do and show:\n",
        "\n",
        "\n",
        "1.   3 Layered ANN implementation\n",
        "2.   Must indicate node amount for each layer\n",
        "1.   Must showcase fiddling around with each node amount\n",
        "2.   Must report accuracy of the network on the validation set.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ECS1nfzkaQQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [ 4, 7, 3, 6,7,9,31,6]\n",
        "\n",
        "plt.hist(x, 2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ti4v7ZWgFb6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import cycle\n",
        "def generate_fake_dataframe(size, cols, col_names = None, intervals = None, seed = None):\n",
        "\n",
        "    categories_dict = {'animals': ['cow', 'rabbit', 'duck', 'shrimp', 'pig', 'goat', 'crab', 'deer', 'bee', 'sheep', 'fish', 'turkey', 'dove', 'chicken', 'horse'],\n",
        "                       'names'  : ['James', 'Mary', 'Robert', 'Patricia', 'John', 'Jennifer', 'Michael', 'Linda', 'William', 'Elizabeth', 'Ahmed', 'Barbara', 'Richard', 'Susan', 'Salomon', 'Juan Luis'],\n",
        "                       'cities' : ['Stockholm', 'Denver', 'Moscow', 'Marseille', 'Palermo', 'Tokyo', 'Lisbon', 'Oslo', 'Nairobi', 'Río de Janeiro', 'Berlin', 'Bogotá', 'Manila', 'Madrid', 'Milwaukee'],\n",
        "                       'colors' : ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'purple', 'pink', 'silver', 'gold', 'beige', 'brown', 'grey', 'black', 'white']\n",
        "                      }\n",
        "    default_intervals = {\"i\" : (0,10), \"f\" : (0,100), \"c\" : (\"names\", 5), \"d\" : (\"2020-01-01\",\"2020-12-31\")}\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    first_c = default_intervals[\"c\"][0]\n",
        "    categories_names = cycle([first_c] + [c for c in categories_dict.keys() if c != first_c])\n",
        "    default_intervals[\"c\"] = (categories_names, default_intervals[\"c\"][1])\n",
        "\n",
        "    if isinstance(col_names,list):\n",
        "        assert len(col_names) == len(cols), f\"The fake DataFrame should have {len(cols)} columns but col_names is a list with {len(col_names)} elements\"\n",
        "    elif col_names is None:\n",
        "        suffix = {\"c\" : \"cat\", \"i\" : \"int\", \"f\" : \"float\", \"d\" : \"date\"}\n",
        "        col_names = [f\"column_{str(i)}_{suffix.get(col)}\" for i, col in enumerate(cols)]\n",
        "\n",
        "    if isinstance(intervals,list):\n",
        "        assert len(intervals) == len(cols), f\"The fake DataFrame should have {len(cols)} columns but intervals is a list with {len(intervals)} elements\"\n",
        "    else:\n",
        "        if isinstance(intervals,dict):\n",
        "            assert len(set(intervals.keys()) - set(default_intervals.keys())) == 0, f\"The intervals parameter has invalid keys\"\n",
        "            default_intervals.update(intervals)\n",
        "        intervals = [default_intervals[col] for col in cols]\n",
        "    df = pd.DataFrame()\n",
        "    for col, col_name, interval in zip(cols, col_names, intervals):\n",
        "        if interval is None:\n",
        "            interval = default_intervals[col]\n",
        "        assert (len(interval) == 2 and isinstance(interval, tuple)) or isinstance(interval, list), f\"This interval {interval} is neither a tuple of two elements nor a list of strings.\"\n",
        "        if col in (\"i\",\"f\",\"d\"):\n",
        "            start, end = interval\n",
        "        if col == \"i\":\n",
        "            df[col_name] = rng.integers(start, end, size)\n",
        "        elif col == \"f\":\n",
        "            df[col_name] = rng.uniform(start, end, size)\n",
        "        elif col == \"c\":\n",
        "            if isinstance(interval, list):\n",
        "                categories = np.array(interval)\n",
        "            else:\n",
        "                cat_family, length = interval\n",
        "                if isinstance(cat_family, cycle):\n",
        "                    cat_family = next(cat_family)\n",
        "                assert cat_family in categories_dict.keys(), f\"There are no samples for category '{cat_family}'. Consider passing a list of samples or use one of the available categories: {categories_dict.keys()}\"\n",
        "                categories = rng.choice(categories_dict[cat_family], length, replace = False, shuffle = True)\n",
        "            df[col_name] = rng.choice(categories, size, shuffle = True)\n",
        "        elif col == \"d\":\n",
        "            df[col_name] = rng.choice(pd.date_range(start, end), size)\n",
        "    return df"
      ],
      "metadata": {
        "id": "28v2x34oHw7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = generate_fake_dataframe(size = 1000, cols =  \"cififficcd\")"
      ],
      "metadata": {
        "id": "aEWfyVXIHzET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# column_0_cat\n",
        "y = []\n",
        "\n",
        "for val in df['column_0_cat']:\n",
        "  y.append(val)\n",
        "\n",
        "print(df['column_0_cat'][0])\n",
        "\n",
        "\n",
        "def stupid_shit():\n",
        "  a = df['column_0_cat']\n",
        "  for val in a:\n",
        "    y.append(val)\n",
        "\n",
        "stupid_shit()\n",
        "print(y)"
      ],
      "metadata": {
        "id": "VWHZmtDvH7gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "K8ur-bQaIMCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'Name': ['Jai', 'Princi', 'Gaurav', 'Anuj'],\n",
        "        'Height': [5.1, 6.2, 5.1, 5.2],\n",
        "        'Qualification': ['Msc', 'MA', 'Msc', 'Msc']}\n",
        "\n",
        "# Convert the dictionary into DataFrame\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "8DAfI6bJJme5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['helloworld'] = 'speed'"
      ],
      "metadata": {
        "id": "h-wBJ6g0Joc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row, col = df.shape\n",
        "print(row)\n",
        "print(col)"
      ],
      "metadata": {
        "id": "BsN2wrUYJrGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TF5vvRIqBcSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "ary = np.arange(0, 25, 1)\n",
        "\n",
        "# Converting the 1 Dimensional array to a 2D array\n",
        "# (to allow explicitly column and row operations)\n",
        "ary = ary.reshape(5, 5)\n",
        "\n",
        "\n",
        "\n",
        "# Displaying the Matrix (use print(ary) in IDE)\n",
        "print(ary)\n",
        "\n",
        "# This for loop will iterate over all columns of the array one at a time\n",
        "\n",
        "# print(ary)\n",
        "\n",
        "for col in range(ary.shape[1]):\n",
        "    # print(f'this is column {col} {ary[:, col]}')\n",
        "    # # interest_row = ary[:,col]\n",
        "    # sum = np.sum(np.exp(ary[:,col]))\n",
        "    # # print(f'the sum of the rows is {sum}')\n",
        "    # e = np.exp(ary[:, col])\n",
        "    # print(f'Post exponentiation this is column {col} {e}')\n",
        "    # answer = np.exp(ary[:, col])/ sum\n",
        "    # print(f'the answer is {answer}')\n",
        "    # # To keep myself organized, I've just done the appropriate math I just need to make sure it reflects back to my original Z!\n",
        "    # # ary[:, col] = answer\n",
        "    # print(f'NOW this is column {col} {ary[:, col]}')\n",
        "    list()\n",
        "print(ary)\n",
        "\n"
      ],
      "metadata": {
        "id": "dslfvbFNQEwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = np.exp(ary)\n",
        "print(e/np.sum(e))"
      ],
      "metadata": {
        "id": "_20dkccTcIut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ary = np.random.randn(10, 2)\n",
        "\n",
        "possible_A2 = ary\n",
        "num_row, num_col = possible_A2.shape\n",
        "\n",
        "y_hat = np.ones(num_row)\n",
        "\n",
        "print(\"can you print\")\n",
        "for i, row in enumerate(possible_A2):\n",
        "  print(\"do i reach here\")\n",
        "  print(row)\n",
        "  # print(row[0])\n",
        "  if row[0] > row[1]: #>\n",
        "    print(f'row 0 is {row[0]}')\n",
        "    print(f'row 1 is {row[1]}')\n",
        "    y_hat[i] = 0"
      ],
      "metadata": {
        "id": "uJH2L3TyHKIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = np.array(y_hat)\n",
        "y_hat.shape"
      ],
      "metadata": {
        "id": "geuSRP_3HU15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "PslGH7_vIoGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na4CpxLBAGGP"
      },
      "source": [
        "- - -\n",
        "## 3. IMPLEMENTING YOUR OWN SIMPLE NEURAL NETWORK\n",
        "\n",
        "Now that you see how the standard library ANN performs on the XOR dataset, you will attempt to implement your own version of the neural network. To help you, a template has been created including the backpropagation. Essensially, you will get the backward gradients for free. However, please note that the backprop implementation assume usage of tanh activation for the hidden layer and softmax for the output layer. There are some subtasks which you need to implement in order to get the network to work properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRRstnXAV77G"
      },
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "\n",
        "class MyNeuralNet(BaseEstimator):\n",
        "    \"\"\"Your implementation of a simple neural network\"\"\"\n",
        "\n",
        "    def __init__ (self, n0, n1, n2, alpha=0.01):\n",
        "        \"\"\"\n",
        "        @param: n0: Number of nodes in the input layer\n",
        "        @param: n1: Number of nodes in the hidden layer\n",
        "        @param: n2: Number of nodes in the output layer\n",
        "        @param: alpha: The Learning Rate\n",
        "        \"\"\"\n",
        "\n",
        "        # SUBTASK 1: Initialize the parameters to random values.\n",
        "        np.random.seed(42)\n",
        "        self.W1 = np.random.randn(n1,n0) #might be rand because can't allow negatives? # is this supposed to be input, output???\n",
        "\n",
        "        # self.W1 = np.random.randn(n0,n1)\n",
        "        self.b1 = np.random.randn(n1,1) #matrix and vector addition reconciled numpy stuff\n",
        "        self.W2 = np.random.randn(n2, n1)\n",
        "\n",
        "        # self.W2 = np.random.randn(n1, n2)\n",
        "        self.b2 = np.random.randn(n2, 1)\n",
        "\n",
        "        # Configure the learning rate\n",
        "        self.alpha = alpha\n",
        "        # One-hot encoder for labels\n",
        "        self.encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "\n",
        "    def tanh(self, z):\n",
        "      a = math.e ** z\n",
        "      b = math.e ** (-z)\n",
        "      return ( (a - b) / (a + b) )\n",
        "\n",
        "    def softmax(self, z):\n",
        "      # for col in range(z.shape[1]):\n",
        "      #   e = np.exp(z[:, col])\n",
        "      # e = np.exp(z)\n",
        "      e = np.exp(z)\n",
        "      return e / e.sum()\n",
        "      # What is i and what is j in these case?\n",
        "\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        \"\"\"\n",
        "        Pass the signal forward through the layers.\n",
        "        @param: X: feature\n",
        "        @return: A1: saved value of the output of the hidden layer\n",
        "        @return: A2: activated return value of the output layer.\n",
        "        \"\"\"\n",
        "        # SUBTASK 2: Implement Forward propagation.\n",
        "        # Note: that you must implement tanh activation for the hiden layer\n",
        "        # and softmax for the output layer\n",
        "\n",
        "        # W1X+B1\n",
        "        # print(f'the shape of W1 is {self.W1.shape}')\n",
        "        # print(f'the shape of X T is {X.T.shape}')\n",
        "        # Z1 = np.dot(self.W1.T, X.T)\n",
        "\n",
        "        Z1 = np.dot(self.W1, X)\n",
        "        Z1 = np.add(Z1, self.b1)\n",
        "\n",
        "        A1 = np.tanh(Z1)\n",
        "        # A1 = A1.T\n",
        "\n",
        "        # W2A1 + B2\n",
        "        # Z2 = np.dot(self.W2.T, A1)\n",
        "\n",
        "        Z2 = np.dot(self.W2, A1)\n",
        "        Z2 = np.add(Z2, self.b2)\n",
        "\n",
        "        A2 = self.softmax(Z2)\n",
        "        # A2 = A2.T\n",
        "\n",
        "        return A1, A2\n",
        "\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Evaluate the total loss on the dataset\n",
        "        @param: X: features\n",
        "        @param: y: labels\n",
        "        @return: L: the loss value\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # SUBTASK 3: Calculate the loss using Cross-Entropy\n",
        "        # You will need to return the average loss on the data\n",
        "        # Hint: Use A2 to calculate the loss\n",
        "        A1, A2 = self.forward_pass(X)\n",
        "        # Another Hint: First, you may want to convert the lable y into a one-hot vector\n",
        "        Y = self.one_hot(y)\n",
        "        p = A2 # is p = A2?\n",
        "        # sum = 0\n",
        "\n",
        "        # print(f'this is Y {Y}')\n",
        "        print(f'the shape of Y is {Y.shape}')\n",
        "        print(f'the shape of A2 is {A2.shape}')\n",
        "\n",
        "        num_row, num_col = A2.shape\n",
        "        sum = 0\n",
        "\n",
        "        first = np.multiply(-Y, np.log2(A2))\n",
        "        oneY = np.subtract(1, Y)\n",
        "        oneP = np.subtract(1, A2)\n",
        "        product = np.multiply(oneY, np.log2(oneP))\n",
        "        L = np.subtract(first, product)\n",
        "        L = np.sum(L)\n",
        "\n",
        "        # for i in range(num_row):\n",
        "        #   one = Y[i] * np.log(A2[i])\n",
        "        #   two = 1-Y[i]\n",
        "        #   three = np.log(1-A2[i])\n",
        "        #   sum = sum + one + two*three\n",
        "\n",
        "        # for i in range(num_row):\n",
        "        #   sum = sum + Y[i]*np.log(p[i])\n",
        "\n",
        "        # L = sum*-1\n",
        "        # L = np.sum(Y * np.log(p)) #don't forget the negative 1\n",
        "        # L = L * -1\n",
        "\n",
        "        # a = np.multiply(-1*Y, np.log(p))\n",
        "        # b = np.multiply((1-Y), np.log(1-p))\n",
        "        # L = np.sum(a-b)\n",
        "# wrong\n",
        "        return L\n",
        "\n",
        "    def backward_pass(self, A1, A2, X, y):\n",
        "        \"\"\"\n",
        "        @param: X: feature\n",
        "        @param: y: label\n",
        "        @param: A1: saved value of the output of the hidden layer\n",
        "        @param: a2: activated return value of the output layer.\n",
        "\n",
        "        @return: dW1: the loss gradient of W1\n",
        "        @return: db1: the loss gradient of b1\n",
        "        @return: dW2: the loss gradient of W2\n",
        "        @return: db2: the loss gradient of b2\n",
        "        \"\"\"\n",
        "\n",
        "        # You DO NOT CHANGE this function, ...\n",
        "        # unless you are advanced and want to use different activation function for your forward pass\n",
        "        # This is an elegant partial derivative of cross entropy with softmax\n",
        "        # Ref document: https://deepnotes.io/softmax-crossentropy\n",
        "        m   = y.shape[0]\n",
        "        dZ2 = A2 - self.one_hot(y)\n",
        "        dW2 = np.dot(dZ2, A1.T)/m\n",
        "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m\n",
        "        dZ1 = np.multiply( np.dot( self.W2.T, dZ2), 1-np.power( A1, 2))\n",
        "        dW1 = np.dot(dZ1, X.T)/m\n",
        "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m\n",
        "\n",
        "        return dW1, db1, dW2, db2\n",
        "\n",
        "\n",
        "    def fit(self, X, y , epochs, X_val, Y_val):\n",
        "        \"\"\"\n",
        "        Learns parameters for the neural network and returns the model.\n",
        "\n",
        "        @param: X: the training feature\n",
        "        @param: y: the train label\n",
        "        @param: epochs: Number of passes through the training data for gradient descent\n",
        "        @param: X_val: the feature of validation set\n",
        "        @param: y_val: the label of validation set\n",
        "        \"\"\"\n",
        "\n",
        "        # Input checks: X and X_val needs to be in the form of n0 x m\n",
        "        if (X.shape[0] > X.shape[1]): X = X.T\n",
        "        if (X_val.shape[0] > X_val.shape[1]): X_val = X_val.T\n",
        "\n",
        "        # Gradient descent\n",
        "        for i in range(0, epochs):\n",
        "\n",
        "            # SUBTASK 4: Compute the forward, backward, and gradient descent parameter update\n",
        "            # Step 1: Forward pass\n",
        "            A1, A2 = self.forward_pass(X)\n",
        "\n",
        "            # Step 2: Backward pass\n",
        "            dW1, db1, dW2, db2 = self.backward_pass(A1, A2, X, y)\n",
        "\n",
        "            # Step 3: Gradient Descent\n",
        "            self.W1 = np.subtract(self.W1, alpha*dW1) #np.subtract(self.W1, alpha*dW1)\n",
        "            self.b1 = np.subtract(self.b1, alpha*db1)\n",
        "            self.W2 = np.subtract(self.W2, alpha * dW2)\n",
        "            self.b2 = np.subtract(self.b2, alpha*db2)\n",
        "\n",
        "            # Print the loss and validation accuracy every 10 epochs.\n",
        "            if i % 10 == 0:\n",
        "                print(\"Epoch %i/%i - loss: %f - accuracy: %f - val_loss: %f - val_accuracy: %f\"\n",
        "                      %(i,epochs, self.loss(X, y), self.evaluate(X, y),\n",
        "                        self.loss(X_val, y_val), self.evaluate(X_val,y_val)))\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict label vector y\n",
        "        \"\"\"\n",
        "        # check X for the form of n0 x m\n",
        "        if (X.shape[0] > X.shape[1]): X = X.T\n",
        "\n",
        "        # SUBTASK 5: Implement the prediction process.\n",
        "        # Hint: It should include a forward pass, and then use the class with higher probability.\n",
        "        # y_hat =\n",
        "        possible_A1, possible_A2 = self.forward_pass(X)\n",
        "        num_row, num_col = possible_A2.shape\n",
        "\n",
        "        y_hat = np.ones(num_col)\n",
        "        print(f'possible_A2 s shape is {possible_A2.shape}')\n",
        "\n",
        "        # for i, row in enumerate(possible_A2):\n",
        "        #   if row[0] > row[1]:\n",
        "        #     y_hat[i] = 0\n",
        "\n",
        "        for i in range(num_row):\n",
        "          if possible_A2[0,i] > possible_A2[1,i]:\n",
        "            y_hat[i] = 0\n",
        "\n",
        "        # print(y_hat)\n",
        "\n",
        "        return y_hat #1x720\n",
        "\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        \"\"\"\n",
        "        Evaluate the accuracy of the model\n",
        "        \"\"\"\n",
        "        m = y.shape[0]\n",
        "        y_hat = self.predict(X)\n",
        "        correct_y = (y_hat == y).astype(int)\n",
        "\n",
        "        # m = y.shape[0]\n",
        "        # y_hat = self.predict(X)\n",
        "        # num_row = y_hat.shape\n",
        "        # correct_y = np.zeros(num_row)\n",
        "        # counter = 0\n",
        "        # print(f'yhat s shape is {y_hat.shape}')\n",
        "        # print(f'y shape is {y.shape}')\n",
        "        # for i in range(len(y.shape)):\n",
        "        #   if (y_hat[i] == y[i]):\n",
        "        #     correct_y[i] = 1\n",
        "        #     counter = counter + 1\n",
        "\n",
        "        return sum(correct_y)/m\n",
        "        # return np.sum(correct_y)/m\n",
        "        # print(f'counter is {counter}')\n",
        "        # return counter/m\n",
        "\n",
        "\n",
        "    def one_hot(self, y):\n",
        "        \"\"\"\n",
        "        Utility function: Convert a label vector to one-hot vector\n",
        "        \"\"\"\n",
        "        Y = self.encoder.fit_transform(y.reshape(len(y),1))\n",
        "        return Y.T # Transpose to get into same shape 1 x m\n",
        "\n",
        "\n",
        "    def plot_decision_boundary(self, X, y):\n",
        "        \"\"\"\n",
        "        Utility Function: Plot a decision boundary for visualization purpose.\n",
        "        If you don't fully understand this function don't worry, it just generates the contour plot below.\n",
        "        \"\"\"\n",
        "        # Set min and max values and give it some padding\n",
        "        x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n",
        "        y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n",
        "        h = 0.01\n",
        "        # Generate a grid of points with distance h between them\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "        # Predict the function value for the whole gid\n",
        "        data_grid= np.c_[xx.ravel(), yy.ravel()]\n",
        "        Z = self.predict(data_grid)\n",
        "        Z = Z.reshape(xx.shape)\n",
        "\n",
        "        # Plot the contour and training examples\n",
        "        plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1)\n",
        "        plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Spectral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train[0])"
      ],
      "metadata": {
        "id": "l9oPGsikARJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n0 = 2\n",
        "# n1 = 6\n",
        "# n2 = 2\n",
        "# alpha = 0.095\n",
        "# epochs = 320 # number of iteration/epochs\n",
        "\n",
        "# myModel1 = MyNeuralNet(n0, n1, n2, alpha)\n",
        "\n",
        "# A1, A2 = myModel1.forward_pass(X_train)\n",
        "\n",
        "# print(A1.shape)\n",
        "# print(A2.shape)\n",
        "# print(y_train.shape)\n",
        "# # I know that A2 and A2 need to have same number of rows as y_train...."
      ],
      "metadata": {
        "id": "DeMRBkklMkJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2srDOZWmM9cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "CCKeAniH_ofU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n0 =  2 #len(X_train[0])\n",
        "n1 = 6\n",
        "n2 = 2\n",
        "alpha = 0.095\n",
        "epochs = 320 # number of iteration/epochs\n",
        "\n",
        "# Build a model with 3 layers\n",
        "myModel1 = MyNeuralNet(n0, n1, n2, alpha);\n",
        "myModel1.fit(X_train, y_train, epochs, X_val, y_val)\n",
        "print(\"Accuracy on the test set is \", myModel1.evaluate(X_test, y_test))\n",
        "\n",
        "# Plot the decision boundary\n",
        "myModel1.plot_decision_boundary(X_train, y_train)"
      ],
      "metadata": {
        "id": "TCG5m-FFFY4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = MyNeuralNet(1,2,3)\n",
        "answer = m.softmax(ary)\n",
        "print(np.sum(answer))"
      ],
      "metadata": {
        "id": "YCUMuCv68Z8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "answer = softmax(ary)\n",
        "print(np.sum(answer))"
      ],
      "metadata": {
        "id": "RZod6OSMi9W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaZo2uu7Q6zn"
      },
      "source": [
        "---\n",
        "## 4. REFLECT ON THE COMPARISON BETWEEN YOUR IMPLEMENTATION TO THE STANDARD LIBRARY\n",
        "Now that you have implemented your own Neural Net class, let's use it! Create at least 3 instances of your Neural Net class, each with a different number of nodes in the hiden layer, tune it with the appropriate learning rate and number of iteration. You will test their performance in the Xor dataset and report the test accuracy metrics for each instance of your neural network.\n",
        "\n",
        "Based on the test accuracy, compare your models with the standard library version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEgYtO8-SPqr"
      },
      "source": [
        "# MODEL 1 here\n",
        "n0 = # input layer dimensionality\n",
        "n1 = # hidden layer dimensionality\n",
        "n2 = # output layer dimensionality\n",
        "alpha = # learning rate for gradient descent\n",
        "epochs = # number of iteration/epochs\n",
        "\n",
        "# Build a model with 3 layers\n",
        "myModel1 = MyNeuralNet(n0, n1, n2, alpha);\n",
        "myModel1.fit(X_train, y_train, epochs, X_val, y_val)\n",
        "print(\"Accuracy on the test set is \", myModel1.evaluate(X_test, y_test))\n",
        "\n",
        "# Plot the decision boundary\n",
        "myModel1.plot_decision_boundary(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xca5c1ayata8"
      },
      "source": [
        "# MODEL 2 here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WT7kZ4VazUL"
      },
      "source": [
        "# MODEL 3 here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnqrQvtia0B4"
      },
      "source": [
        "# COMPARISONS WITH THE STANDARD LIBRARY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2-YIcXYc96V"
      },
      "source": [
        "## Reflection\n",
        "Write at least a paragraph answering these prompts: How did your own network perform? Is there any major differences between the implementations? Finally, reflecting on your experience implementing a learning algorithm for this assignment (Was it hard/easy/fun?, From which part did you learn the most?)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0vuIEBDAGGa"
      },
      "source": [
        "---\n",
        "# Get Help?\n",
        "In case you get stuck in any step in the process, you may find some useful information from:\n",
        "\n",
        " * Consult my [lecture 13](https://docs.google.com/presentation/d/1otQfmMomWctLZKI3hHKAA4lLkbXFtagLaQov8gNh4LI/edit?usp=sharing) and [Colab Notebook 10](https://colab.research.google.com/drive/1x5biI3dP5YvvDEI0wapJcSgQNnATDzNe)\n",
        " * Talk to the TA, they are available and there to help you during office hour.\n",
        " * Come talk to me or email me <nn4pj@virginia.edu> with subject starting \"CS4774 Assignment 4:...\".\n",
        "\n",
        "Part of the codes used in this assignment is modified from Konstantinos Kitsios under the 3-Clause BSD License. Best of luck and have fun!"
      ]
    }
  ]
}